Loading datasets...
Restoring previous...
Epoch 1/200
Train set...
  0%|[35m                                                                           [39m| 0/3193 [01:21<?, ?it/s]
Traceback (most recent call last):
  File "/home/tbarba/projects/MultiModalBrainSurvival/src/autoencoder/train_ae.py", line 245, in <module>
    main(**config)
  File "/home/tbarba/projects/MultiModalBrainSurvival/src/autoencoder/train_ae.py", line 209, in main
    train_epoch_metrics = train_loop(
  File "/home/tbarba/projects/MultiModalBrainSurvival/src/autoencoder/train_ae.py", line 44, in train_loop
    outputs = model(images)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tbarba/projects/MultiModalBrainSurvival/src/autoencoder/newmods.py", line 158, in forward
    return self.decode(z), z, (mu, log_var)
  File "/home/tbarba/projects/MultiModalBrainSurvival/src/autoencoder/newmods.py", line 140, in decode
    result = self.final_layer(result)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 613, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/labs/gevaertlab/users/thomas/miniconda/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 608, in _conv_forward
    return F.conv3d(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.25 GiB (GPU 0; 15.77 GiB total capacity; 1.68 GiB already allocated; 12.89 GiB free; 1.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF